{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#2ë²ˆ. ë‹¹ì§„-ì„œë²„ê°„ duetì—°ê²°\n",
    "import syft as sy\n",
    "duet1 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback Transaction.__retry()\n",
      "handle: <TimerHandle when=10910.593 Transaction.__retry()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\aioice\\stun.py\", line 306, in __retry\n",
      "    self.__future.set_exception(TransactionTimeout())\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\asyncio\\futures.py\", line 270, in set_exception\n",
      "    raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n",
      "asyncio.exceptions.InvalidStateError: FINISHED: <Future finished result=(Message(messa...c\\xb2'\\xcas2\"), ('192.168.10.26', 49362))>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#4ë²ˆ. ìš¸ì‚°-ì„œë²„ê°„ duetì—°ê²°\n",
    "duet2 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9ë²ˆ\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10ë²ˆ. ì‚¬ìš©ë°ì´í„°ì˜ ì „ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "# ë‹¹ì§„\n",
    "\n",
    "#ì‚¬ìš©í•  ë°ì´í„°(ê¸°ìƒìš”ì†Œ)ë¥¼ ë¶ˆëŸ¬ì™€ì¤ë‹ˆë‹¤.\n",
    "dangjin_obs_data_path = r'C:dangjin_obs_data.csv'\n",
    "dangjin_obs_data = pd.read_csv(dangjin_obs_data_path)\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "#ë°ì´í„°í”„ë ˆìž„ì— ëˆ„ë½ëœ 6íšŒë¶„ì˜ ì‹œê°„ëŒ€ë¥¼ nanê°’ìœ¼ë¡œ ì±„ì›Œì¤ë‹ˆë‹¤.\n",
    "dangjin_obs_data.loc[3490.1]=[129,'ì„œì‚°','2018-07-24 11:00',np.NaN,np.NaN,np.NaN,np.NaN,np.NaN]\n",
    "dangjin_obs_data.loc[3490.2]=[129,'ì„œì‚°','2018-07-24 12:00',np.NaN,np.NaN,np.NaN,np.NaN,np.NaN]\n",
    "dangjin_obs_data.loc[3490.3]=[129,'ì„œì‚°','2018-07-24 13:00',np.NaN,np.NaN,np.NaN,np.NaN,np.NaN]\n",
    "dangjin_obs_data.loc[3490.4]=[129,'ì„œì‚°','2018-07-24 14:00',np.NaN,np.NaN,np.NaN,np.NaN,np.NaN]\n",
    "dangjin_obs_data.loc[3490.5]=[129,'ì„œì‚°','2018-07-24 15:00',np.NaN,np.NaN,np.NaN,np.NaN,np.NaN]\n",
    "dangjin_obs_data.loc[3490.6]=[129,'ì„œì‚°','2018-07-24 16:00',np.NaN,np.NaN,np.NaN,np.NaN,np.NaN]\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "#6ê°œì˜ í–‰ì´ ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆìž„ì˜ ì¸ë±ìŠ¤ë¥¼ ì •ìˆ˜í™”í•´ì¤ë‹ˆë‹¤\n",
    "dangjin_obs_data = dangjin_obs_data.sort_index() \n",
    "dangjin_obs_data = dangjin_obs_data.reset_index(drop = True)\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "#ë°ì´í„°í”„ë ˆìž„ì— ëˆ„ë½ëœ ë¶€ë¶„ì„ ì„ í˜•ë³´ê°„ë²•ìœ¼ë¡œ ì±„ì›Œì¤ë‹ˆë‹¤.\n",
    "new_dangjin_obs_data=dangjin_obs_data.interpolate(method='values',limit_direction='backward')\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "#ê¸°ìƒìš”ì†Œë¥¼ ì •ê·œí™”í•˜ê³ , noiseê°€ ì¶”ê°€ëœ ìƒˆë¡œìš´ ë°ì´í„° ì…‹ë„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "mu = 0\n",
    "ratio=15\n",
    "\n",
    "tem_dangjin = new_dangjin_obs_data['ê¸°ì˜¨(Â°C)'].values\n",
    "wv_dangjin = new_dangjin_obs_data['í’ì†(m/s)'].values\n",
    "wd_dangjin = new_dangjin_obs_data['í’í–¥(16ë°©ìœ„)'].values\n",
    "humi_dangjin = new_dangjin_obs_data['ìŠµë„(%)'].values\n",
    "cloud_dangjin = new_dangjin_obs_data['ì „ìš´ëŸ‰(10ë¶„ìœ„)'].values\n",
    "\n",
    "sigma_tem_dangjin = np.max(tem_dangjin)*(ratio/100)\n",
    "sigma_wv_dangjin = np.max(wv_dangjin)*(ratio/100)\n",
    "sigma_wd_dangjin = np.max(wd_dangjin)*(ratio/100)\n",
    "sigma_humi_dangjin = np.max(humi_dangjin)*(ratio/100)\n",
    "sigma_cloud_dangjin = np.max(cloud_dangjin)*(ratio/100)\n",
    "\n",
    "tem_noise_dangjin = np.random.normal(mu, sigma_tem_dangjin, tem_dangjin.shape) \n",
    "wv_noise_dangjin = np.random.normal(mu, sigma_wv_dangjin, wv_dangjin.shape)  \n",
    "wd_noise_dangjin = np.random.normal(mu, sigma_wd_dangjin, wd_dangjin.shape)  \n",
    "humi_noise_dangjin = np.random.normal(mu, sigma_humi_dangjin, humi_dangjin.shape)  \n",
    "cloud_noise_dangjin = np.random.normal(mu, sigma_cloud_dangjin, cloud_dangjin.shape)  \n",
    "#-----------------------------------------------------------------------------------------------\n",
    "data_tem_noise_dangjin = np.reshape(np.array(tem_dangjin + tem_noise_dangjin),(1068,24))/np.max(tem_dangjin)\n",
    "data_wv_noise_dangjin = np.reshape(np.array(wv_dangjin + wv_noise_dangjin),(1068,24))/np.max(wv_dangjin)\n",
    "data_wd_noise_dangjin = np.reshape(np.array(wd_dangjin + wd_noise_dangjin),(1068,24))/np.max(wd_dangjin)\n",
    "data_humi_noise_dangjin = np.reshape(np.array(humi_dangjin + humi_noise_dangjin),(1068,24))/np.max(humi_dangjin)\n",
    "data_cloud_noise_dangjin = np.reshape(np.array(cloud_dangjin + cloud_noise_dangjin),(1068,24))/np.max(cloud_dangjin)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "data_tem_dangjin = np.reshape(tem_dangjin,(1068,24))/np.max(tem_dangjin)\n",
    "data_wv_dangjin = np.reshape(wv_dangjin,(1068,24))/np.max(wv_dangjin)\n",
    "data_wd_dangjin = np.reshape(wd_dangjin,(1068,24))/np.max(wd_dangjin)\n",
    "data_humi_dangjin = np.reshape(humi_dangjin,(1068,24))/np.max(humi_dangjin)\n",
    "data_cloud_dangjin = np.reshape(cloud_dangjin,(1068,24))/np.max(cloud_dangjin)\n",
    "\n",
    "Input_dangjin = np.concatenate((data_tem_dangjin,data_wv_dangjin,data_wd_dangjin, data_humi_dangjin, data_cloud_dangjin ), axis=1)\n",
    "Input_noise_dangjin = np.concatenate((data_tem_noise_dangjin,data_wv_noise_dangjin,data_wd_noise_dangjin, data_humi_noise_dangjin, data_cloud_noise_dangjin ), axis=1)\n",
    "\n",
    "# ì‚¬ìš©ë°ì´í„°ë¥¼ íŠ¸ë ˆì´ë‹,ë°œë¦¬ë°ì´ì…˜,í…ŒìŠ¤íŠ¸ ì…‹ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "train_len=848\n",
    "valid_len=110\n",
    "\n",
    "train_input_dangjin = Input_dangjin[:train_len,:]\n",
    "valid_input_dangjin = Input_dangjin[train_len:train_len+valid_len,:]\n",
    "test_input_dangjin = Input_noise_dangjin[train_len+valid_len:,:]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "# ìš¸ì‚°\n",
    "\n",
    "#ì‚¬ìš©í•  ë°ì´í„°(ê¸°ìƒìš”ì†Œ)ë¥¼ ë¶ˆëŸ¬ì™€ì¤ë‹ˆë‹¤.\n",
    "ulsan_obs_data_path = r'C:ulsan_obs_data.csv'\n",
    "ulsan_obs_data = pd.read_csv(ulsan_obs_data_path)\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "#ë°ì´í„°í”„ë ˆìž„ì— ëˆ„ë½ëœ ë¶€ë¶„ì„ ì„ í˜•ë³´ê°„ë²•ìœ¼ë¡œ ì±„ì›Œì¤ë‹ˆë‹¤.\n",
    "new_ulsan_obs_data=ulsan_obs_data.interpolate(method='values',limit_direction='backward')\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "#ê¸°ìƒìš”ì†Œë¥¼ ì •ê·œí™”í•˜ê³ , noiseê°€ ì¶”ê°€ëœ ìƒˆë¡œìš´ ë°ì´í„° ì…‹ë„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "tem_ulsan = new_ulsan_obs_data['ê¸°ì˜¨(Â°C)'].values\n",
    "wv_ulsan = new_ulsan_obs_data['í’ì†(m/s)'].values\n",
    "wd_ulsan = new_ulsan_obs_data['í’í–¥(16ë°©ìœ„)'].values\n",
    "humi_ulsan = new_ulsan_obs_data['ìŠµë„(%)'].values\n",
    "cloud_ulsan = new_ulsan_obs_data['ì „ìš´ëŸ‰(10ë¶„ìœ„)'].values\n",
    "\n",
    "sigma_tem_ulsan = np.max(tem_ulsan)*(ratio/100)\n",
    "sigma_wv_ulsan = np.max(wv_ulsan)*(ratio/100)\n",
    "sigma_wd_ulsan = np.max(wd_ulsan)*(ratio/100)\n",
    "sigma_humi_ulsan = np.max(humi_ulsan)*(ratio/100)\n",
    "sigma_cloud_ulsan = np.max(cloud_ulsan)*(ratio/100)\n",
    "\n",
    "tem_noise_ulsan = np.random.normal(mu, sigma_tem_ulsan, tem_ulsan.shape)  # creating a noise with the same dimension as the dataset\n",
    "wv_noise_ulsan = np.random.normal(mu, sigma_wv_ulsan, wv_ulsan.shape)  # creating a noise with the same dimension as the dataset\n",
    "wd_noise_ulsan = np.random.normal(mu, sigma_wd_ulsan, wd_ulsan.shape)  # creating a noise with the same dimension as the dataset\n",
    "humi_noise_ulsan = np.random.normal(mu, sigma_humi_ulsan, humi_ulsan.shape)  # creating a noise with the same dimension as the dataset\n",
    "cloud_noise_ulsan = np.random.normal(mu, sigma_cloud_ulsan, cloud_ulsan.shape)  # creating a noise with the same dimension as the dataset\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "data_tem_noise_ulsan = np.reshape(np.array(tem_ulsan + tem_noise_ulsan),(1068,24))/np.max(tem_ulsan)\n",
    "data_wv_noise_ulsan = np.reshape(np.array(wv_ulsan + wv_noise_ulsan),(1068,24))/np.max(wv_ulsan)\n",
    "data_wd_noise_ulsan = np.reshape(np.array(wd_ulsan + wd_noise_ulsan),(1068,24))/np.max(wd_ulsan)\n",
    "data_humi_noise_ulsan = np.reshape(np.array(humi_ulsan + humi_noise_ulsan),(1068,24))/np.max(humi_ulsan)\n",
    "data_cloud_noise_ulsan = np.reshape(np.array(cloud_ulsan + cloud_noise_ulsan),(1068,24))/np.max(cloud_ulsan)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "data_tem_ulsan = np.reshape(tem_ulsan,(1068,24))/np.max(tem_ulsan)\n",
    "data_wv_ulsan = np.reshape(wv_ulsan,(1068,24))/np.max(wv_ulsan)\n",
    "data_wd_ulsan = np.reshape(wd_ulsan,(1068,24))/np.max(wd_ulsan)\n",
    "data_humi_ulsan = np.reshape(humi_ulsan,(1068,24))/np.max(humi_ulsan)\n",
    "data_cloud_ulsan = np.reshape(cloud_ulsan,(1068,24))/np.max(cloud_ulsan)\n",
    "\n",
    "Input_ulsan = np.concatenate((data_tem_ulsan,data_wv_ulsan,data_wd_ulsan, data_humi_ulsan, data_cloud_ulsan ), axis=1)\n",
    "Input_noise_ulsan = np.concatenate((data_tem_noise_ulsan,data_wv_noise_ulsan,data_wd_noise_ulsan, data_humi_noise_ulsan, data_cloud_noise_ulsan ), axis=1)\n",
    "\n",
    "#ì‚¬ìš©ë°ì´í„°ë¥¼ íŠ¸ë ˆì´ë‹,ë°œë¦¬ë°ì´ì…˜,í…ŒìŠ¤íŠ¸ ì…‹ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "\n",
    "train_input_ulsan = Input_ulsan[:train_len,:]\n",
    "valid_input_ulsan = Input_ulsan[train_len:train_len+valid_len,:]\n",
    "test_input_ulsan = Input_noise_ulsan[train_len+valid_len:,:]\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11ë²ˆ. ë‘ í´ë¼ì´ì–¸íŠ¸ê°€ pysyftì— ì˜¬ë¦° ë°ì´í„°ë¥¼ í¬ì¸í„°í˜•íƒœë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "train_out_ptr_dangjin=duet1.store[0]\n",
    "valid_out_ptr_dangjin=duet1.store[1]\n",
    "test_out_ptr_dangjin=duet1.store[2]\n",
    "\n",
    "train_out_ptr_ulsan=duet2.store[0]\n",
    "valid_out_ptr_ulsan=duet2.store[1]\n",
    "test_out_ptr_ulsan=duet2.store[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12ë²ˆ. í•™ìŠµì— ì‚¬ìš©í•  ëª¨ë¸ì„ ìž‘ì„±í•©ë‹ˆë‹¤.\n",
    "in_size        = 120\n",
    "out_size       = 24\n",
    "train_len      = 848\n",
    "hidden_size    = 256\n",
    "batch_size     = 32\n",
    "learning_rate  = 1e-4\n",
    "\n",
    "total_epoch    = 100\n",
    "print_interval = 50\n",
    "\n",
    "class MLP(sy.Module):\n",
    "    def __init__(self,torch_ref):\n",
    "        super(MLP, self).__init__(torch_ref=torch_ref)\n",
    "        self.fc_in  = self.torch_ref.nn.Linear(in_size, hidden_size)\n",
    "        self.fc_h1  = self.torch_ref.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_h2  = self.torch_ref.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_h3  = self.torch_ref.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = self.torch_ref.nn.Linear(hidden_size, out_size)\n",
    "\n",
    "        self.torch_ref.torch.nn.init.xavier_uniform_(self.fc_in.weight)\n",
    "        self.torch_ref.torch.nn.init.xavier_uniform_(self.fc_h1.weight)\n",
    "        self.torch_ref.torch.nn.init.xavier_uniform_(self.fc_h2.weight)\n",
    "        self.torch_ref.torch.nn.init.xavier_uniform_(self.fc_h3.weight)\n",
    "        self.torch_ref.torch.nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.torch_ref.nn.functional.relu(self.fc_in(x))\n",
    "        x = self.torch_ref.nn.functional.relu(self.fc_h1(x))\n",
    "        x = self.torch_ref.nn.functional.relu(self.fc_h2(x))\n",
    "        x = self.torch_ref.nn.functional.relu(self.fc_h3(x))\n",
    "        out = self.torch_ref.nn.functional.relu(self.fc_out(x))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pdb\n",
    "def local_train(epoch, model, torch_ref, optim, train_input_ptr, train_output_ptr, batch_size):\n",
    "    \n",
    "    losses = []\n",
    "    length = train_input_ptr.shape[0]\n",
    "    index_list = torch.randperm(length)\n",
    "    criterion = torch_ref.nn.L1Loss()\n",
    "    \n",
    "    before = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx in range(1,length//batch_size+1):\n",
    "        \n",
    "        batch = batch_size * batch_idx\n",
    "        if batch >= length:\n",
    "            batch = length -1\n",
    "            \n",
    "        selected_idx = index_list[before:batch]\n",
    "        \n",
    "        optim.zero_grad()\n",
    "      \n",
    "        data = torch_ref.FloatTensor(train_input_ptr[selected_idx,:])\n",
    "        target = train_output_ptr[selected_idx,:]\n",
    "\n",
    "        out = model(data)\n",
    "        \n",
    "        loss = criterion(out, target)\n",
    "        loss = loss/torch_ref.mean(target)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        loss_item = loss.item()\n",
    "        \n",
    "        loss_value = loss_item.get(\n",
    "            reason=\"To evaluate training progress\",\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "        )\n",
    "\n",
    "        #print(\"Epoch\", batch_idx, \"loss\", loss_value)\n",
    "        \n",
    "        losses.append(loss_value)\n",
    "        before = batch\n",
    "        \n",
    "    loss_avg = np.mean(losses)\n",
    "    \n",
    "    return loss_avg, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_valid(epoch, model, torch_ref, optim, valid_input_ptr, valid_output_ptr, batch_size):\n",
    "    \n",
    "    losses = []\n",
    "    length = valid_input_ptr.shape[0]\n",
    "    index_list = torch.randperm(length)\n",
    "    criterion = torch_ref.nn.L1Loss()\n",
    "    before = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(1,length//batch_size+1):\n",
    "\n",
    "            batch = batch_size * batch_idx\n",
    "            if batch >= length:\n",
    "                batch = length -1\n",
    "\n",
    "            selected_idx = index_list[before:batch]\n",
    "\n",
    "            data = torch_ref.FloatTensor(valid_input_ptr[selected_idx,:])\n",
    "            target = valid_output_ptr[selected_idx,:]\n",
    "\n",
    "            out = model(data)\n",
    "            \n",
    "            loss = criterion(out, target)\n",
    "            loss = loss/torch_ref.mean(target)\n",
    "       \n",
    "    \n",
    "            loss_item = loss.item()\n",
    "\n",
    "            loss_value = loss_item.get(\n",
    "                reason=\"To evaluate training progress\",\n",
    "                request_block=True,\n",
    "                timeout_secs=5,\n",
    "            )\n",
    "\n",
    "            #print(\"Epoch\", batch_idx, \"loss\", loss_value)\n",
    "\n",
    "            losses.append(loss_value)\n",
    "            before = batch\n",
    "\n",
    "    loss_avg = np.mean(losses)\n",
    "        \n",
    "    return loss_avg, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, torch_ref,test_input_ptr, test_output_ptr):\n",
    "    \n",
    "    losses = []\n",
    "    length = test_input_ptr.shape[0]\n",
    "    index_list = torch.randperm(length)\n",
    "    criterion = torch_ref.nn.L1Loss()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "\n",
    "        data = torch_ref.FloatTensor(test_input_ptr)\n",
    "        target = test_output_ptr\n",
    "\n",
    "        out = model(data)\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        loss = loss/torch_ref.mean(target)\n",
    "\n",
    "        loss_item = loss.item()\n",
    "\n",
    "        loss_value = loss_item.get(\n",
    "            reason=\"To evaluate training progress\",\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "        )\n",
    "\n",
    "        #print(\"Epoch\", batch_idx, \"loss\", loss_value)\n",
    "\n",
    "        losses.append(loss_value)\n",
    "\n",
    "    loss_avg = np.mean(losses)\n",
    "        \n",
    "    return loss_avg, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def federated_traing(local_train, epochs,duet1,duet2, \n",
    "                     train_input1, train_output1, \n",
    "                     train_input2, train_output2,\n",
    "                     valid_input1, valid_output1, \n",
    "                     valid_input2, valid_output2,\n",
    "                     test_input1, test_input2, \n",
    "                     test_output1, test_output2,\n",
    "                     batch_size,option):\n",
    "\n",
    "    global_model = MLP(torch)\n",
    "    \n",
    "    remote_model1 = global_model.send(duet1)  #ë‹¹ì§„\n",
    "    remote_torch1 = duet1.torch\n",
    "    params1 = remote_model1.parameters()\n",
    "    optim1 = remote_torch1.optim.Adam(params=params1, lr=1e-4)\n",
    "    \n",
    "    remote_model2 = global_model.send(duet2)  #ìš¸ì‚°\n",
    "    remote_torch2 = duet2.torch\n",
    "    params2 = remote_model2.parameters()\n",
    "    optim2 = remote_torch2.optim.Adam(params=params2, lr=1e-4)\n",
    "    \n",
    "    train_loss1_list = []\n",
    "    valid_loss1_list = []\n",
    "    \n",
    "    train_loss2_list = []\n",
    "    valid_loss2_list = []\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        print(\"-----------Start Epoch\",epoch,\"-----------\")\n",
    "        \n",
    "        losses1, T_loss1_list = local_train(epoch, remote_model1, remote_torch1, optim1, train_input1, train_output1, 8)\n",
    "        valid_loss1,V_loss1_list  = local_valid(epoch, remote_model1, remote_torch1, optim1, valid_input1, valid_output1, 8)\n",
    "        \n",
    "        print(\"Epoch\",epoch, \"Train Loss1 = \" ,losses1)\n",
    "        print(\"Epoch\",epoch, \"Valid Loss1 = \" ,valid_loss1)\n",
    "        \n",
    "        train_loss1_list.extend(T_loss1_list)\n",
    "        valid_loss1_list.extend(V_loss1_list)\n",
    "\n",
    "        \n",
    "        losses2, T_loss2_list = local_train(epoch, remote_model2, remote_torch2, optim2, train_input2, train_output2,8)\n",
    "        valid_loss2, V_loss2_list  = local_valid(epoch, remote_model2, remote_torch2, optim2, valid_input2, valid_output2, 8)\n",
    "        \n",
    "        print(\"Epoch\",epoch, \"Train Loss2 = \" ,losses2)\n",
    "        print(\"Epoch\",epoch, \"Valid Loss2 = \" ,valid_loss2)\n",
    "        \n",
    "        train_loss2_list.extend(T_loss2_list)\n",
    "        valid_loss2_list.extend(V_loss2_list)\n",
    "        \n",
    "        del T_loss1_list, T_loss2_list, V_loss1_list, V_loss2_list\n",
    "        \n",
    "        if option == \"FL\":\n",
    "            if epoch % 4 == 0:\n",
    "                remote_model1_updates = remote_model1.get(request_block=True).state_dict()\n",
    "                remote_model2_updates = remote_model2.get(request_block=True).state_dict()\n",
    "\n",
    "                avg_updates = OrderedDict()\n",
    "                print(\"-----------Updating Parameters-----------\")\n",
    "\n",
    "                for weight in remote_model1_updates.keys():\n",
    "                    avg_updates[weight] = (remote_model1_updates[weight] + remote_model2_updates[weight]) / 2\n",
    "\n",
    "                updated_model = MLP(torch)\n",
    "                updated_model.load_state_dict(avg_updates)\n",
    "\n",
    "                print(\"-----------Updating Clients-----------\")\n",
    "                remote_model1 = updated_model.send(duet1) \n",
    "                remote_model2 = updated_model.send(duet2) \n",
    "                print(\"-----------Done-----------\")\n",
    "\n",
    "                params1 = remote_model1.parameters()\n",
    "                optim1 = remote_torch1.optim.Adam(params=params1, lr=1e-4)\n",
    "\n",
    "                params2 = remote_model2.parameters()\n",
    "                optim2 = remote_torch2.optim.Adam(params=params2, lr=1e-4)\n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "                \n",
    "    test_losses1, test_loss1_list = test(epoch, remote_model1, remote_torch1,test_input1, test_output2)\n",
    "    print(\"-----\",epoch, \"Test Loss1 = \" ,test_losses1)\n",
    "    \n",
    "    test_losses2, test_loss2_list = test(epoch, remote_model2, remote_torch2,test_input2, test_output2)       \n",
    "    print(\"-----\",epoch, \"Test Loss2 = \" ,test_losses2)\n",
    "          \n",
    "    return train_loss1_list, valid_loss1_list, train_loss2_list, valid_loss2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Start Epoch 1 -----------\n",
      "Epoch 1 Train Loss1 =  0.46718284417435807\n",
      "Epoch 1 Valid Loss1 =  0.4689490840985225\n",
      "Epoch 1 Train Loss2 =  0.48074214090153855\n",
      "Epoch 1 Valid Loss2 =  0.48506843470610106\n",
      "-----------Start Epoch 2 -----------\n",
      "Epoch 2 Train Loss1 =  0.293997548661142\n",
      "Epoch 2 Valid Loss1 =  0.39589530114944166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback Transaction.__retry()\n",
      "handle: <TimerHandle when=11249.359 Transaction.__retry()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\aioice\\stun.py\", line 306, in __retry\n",
      "    self.__future.set_exception(TransactionTimeout())\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\asyncio\\futures.py\", line 270, in set_exception\n",
      "    raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n",
      "asyncio.exceptions.InvalidStateError: FINISHED: <Future finished result=(Message(messa...xc2\\xf0\\xe94'), ('192.168.10.26', 49362))>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss2 =  0.31585228358799555\n",
      "Epoch 2 Valid Loss2 =  0.47112088937025803\n",
      "-----------Start Epoch 3 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback Transaction.__retry()\n",
      "handle: <TimerHandle when=11345.453 Transaction.__retry()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\aioice\\stun.py\", line 306, in __retry\n",
      "    self.__future.set_exception(TransactionTimeout())\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\asyncio\\futures.py\", line 270, in set_exception\n",
      "    raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n",
      "asyncio.exceptions.InvalidStateError: FINISHED: <Future finished result=(Message(messa...{\\xe5pN-\\xbb'), ('192.168.10.26', 64513))>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss1 =  0.26457991352621113\n",
      "Epoch 3 Valid Loss1 =  0.3692247821734502\n",
      "Epoch 3 Train Loss2 =  0.28924849053036494\n",
      "Epoch 3 Valid Loss2 =  0.4940522863314702\n",
      "-----------Start Epoch 4 -----------\n",
      "Epoch 4 Train Loss1 =  0.24763044321312094\n",
      "Epoch 4 Valid Loss1 =  0.38875911786006045\n",
      "Epoch 4 Train Loss2 =  0.27072175999857345\n",
      "Epoch 4 Valid Loss2 =  0.5068415999412537\n",
      "-----------Start Epoch 5 -----------\n",
      "Epoch 5 Train Loss1 =  0.2346271515454886\n",
      "Epoch 5 Valid Loss1 =  0.3369313386770395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback Transaction.__retry()\n",
      "handle: <TimerHandle when=11908.437 Transaction.__retry()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\aioice\\stun.py\", line 306, in __retry\n",
      "    self.__future.set_exception(TransactionTimeout())\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\asyncio\\futures.py\", line 270, in set_exception\n",
      "    raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n",
      "asyncio.exceptions.InvalidStateError: FINISHED: <Future finished result=(Message(messa...04GD\\xa2\\x0f'), ('192.168.10.26', 49362))>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss2 =  0.2583086704308132\n",
      "Epoch 5 Valid Loss2 =  0.4541304398041505\n",
      "-----------Start Epoch 6 -----------\n",
      "Epoch 6 Train Loss1 =  0.22578004958494655\n",
      "Epoch 6 Valid Loss1 =  0.3447674478475864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-99699' coro=<RTCSctpTransport._transmit() done, defined at C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\aiortc\\rtcsctptransport.py:1501> exception=ConnectionError('Cannot send encrypted data, not connected')>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\aiortc\\rtcsctptransport.py\", line 1535, in _transmit\n",
      "    await self._send_chunk(chunk)\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\aiortc\\rtcsctptransport.py\", line 1335, in _send_chunk\n",
      "    await self.__transport._send_data(\n",
      "  File \"C:\\Users\\dhkim\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\aiortc\\rtcdtlstransport.py\", line 659, in _send_data\n",
      "    raise ConnectionError(\"Cannot send encrypted data, not connected\")\n",
      "ConnectionError: Cannot send encrypted data, not connected\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m----> 4\u001b[0m train_loss1_list, valid_loss1_list, train_loss2_list, valid_loss2_list \u001b[38;5;241m=\u001b[39m \u001b[43mfederated_traing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mduet1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mduet2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                                                          \u001b[49m\u001b[43mtrain_input_dangjin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_out_ptr_dangjin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                                                          \u001b[49m\u001b[43mtrain_input_ulsan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_out_ptr_ulsan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                                                          \u001b[49m\u001b[43mvalid_input_dangjin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_out_ptr_dangjin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                                                          \u001b[49m\u001b[43mvalid_input_ulsan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_out_ptr_ulsan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                                                                          \u001b[49m\u001b[43mtest_input_dangjin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_out_ptr_dangjin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                                                                          \u001b[49m\u001b[43mtest_input_ulsan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_out_ptr_ulsan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                                                                          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mfederated_traing\u001b[1;34m(local_train, epochs, duet1, duet2, train_input1, train_output1, train_input2, train_output2, valid_input1, valid_output1, valid_input2, valid_output2, test_input1, test_input2, test_output1, test_output2, batch_size, option)\u001b[0m\n\u001b[0;32m     39\u001b[0m valid_loss1_list\u001b[38;5;241m.\u001b[39mextend(V_loss1_list)\n\u001b[0;32m     42\u001b[0m losses2, T_loss2_list \u001b[38;5;241m=\u001b[39m local_train(epoch, remote_model2, remote_torch2, optim2, train_input2, train_output2,\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m valid_loss2, V_loss2_list  \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_model2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_torch2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_input2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_output2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss2 = \u001b[39m\u001b[38;5;124m\"\u001b[39m ,losses2)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid Loss2 = \u001b[39m\u001b[38;5;124m\"\u001b[39m ,valid_loss2)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mlocal_valid\u001b[1;34m(epoch, model, torch_ref, optim, valid_input_ptr, valid_output_ptr, batch_size)\u001b[0m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m/\u001b[39mtorch_ref\u001b[38;5;241m.\u001b[39mmean(target)\n\u001b[0;32m     28\u001b[0m loss_item \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 30\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss_item\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTo evaluate training progress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_secs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#print(\"Epoch\", batch_idx, \"loss\", loss_value)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss_value)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\syft\\core\\pointer\\pointer.py:271\u001b[0m, in \u001b[0;36mPointer.get\u001b[1;34m(self, request_block, timeout_secs, reason, delete_obj, verbose)\u001b[0m\n\u001b[0;32m    269\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(delete_obj\u001b[38;5;241m=\u001b[39mdelete_obj, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     response_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreason\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_secs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_secs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    278\u001b[0m         response_status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m response_status \u001b[38;5;241m==\u001b[39m RequestStatus\u001b[38;5;241m.\u001b[39mAccepted\n\u001b[0;32m    280\u001b[0m     ):\n\u001b[0;32m    281\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(delete_obj\u001b[38;5;241m=\u001b[39mdelete_obj, verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\syft\\core\\pointer\\pointer.py:473\u001b[0m, in \u001b[0;36mPointer.request\u001b[1;34m(self, reason, block, timeout_secs, verbose)\u001b[0m\n\u001b[0;32m    467\u001b[0m debug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Sending another Request Message \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow \u001b[38;5;241m-\u001b[39m start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    468\u001b[0m status_msg \u001b[38;5;241m=\u001b[39m RequestAnswerMessage(\n\u001b[0;32m    469\u001b[0m     request_id\u001b[38;5;241m=\u001b[39mmsg\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m    470\u001b[0m     address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39maddress,\n\u001b[0;32m    471\u001b[0m     reply_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39maddress,\n\u001b[0;32m    472\u001b[0m )\n\u001b[1;32m--> 473\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_immediate_msg_with_reply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatus_msg\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m status \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m RequestStatus\u001b[38;5;241m.\u001b[39mPending:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\syft\\core\\node\\common\\client.py:228\u001b[0m, in \u001b[0;36mClient.send_immediate_msg_with_reply\u001b[1;34m(self, msg, route_index)\u001b[0m\n\u001b[0;32m    225\u001b[0m     debug(output)\n\u001b[0;32m    226\u001b[0m     msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39msign(signing_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigning_key)\n\u001b[1;32m--> 228\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroutes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mroute_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_immediate_msg_with_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mis_valid:\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# check if we have an ExceptionMessage to trigger a local exception\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;66;03m# from a remote exception that we caused\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mmessage, ExceptionMessage):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\syft\\core\\io\\route.py:177\u001b[0m, in \u001b[0;36mSoloRoute.send_immediate_msg_with_reply\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_immediate_msg_with_reply\u001b[39m(\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m, msg: SignedImmediateSyftMessageWithReply\n\u001b[0;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SignedImmediateSyftMessageWithoutReply:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_immediate_msg_with_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\syft\\grid\\connections\\webrtc.py:517\u001b[0m, in \u001b[0;36mWebRTCConnection.send_immediate_msg_with_reply\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03mSends high priority messages and wait for their responses.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m:return: returns an instance of SignedImmediateSyftMessageWithReply.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;124;03m:rtype: SignedImmediateSyftMessageWithReply\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# properly fix this!\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_type(\n\u001b[1;32m--> 517\u001b[0m         \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_sync_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    521\u001b[0m     traceback_and_raise(e)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     33\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\nest_asyncio.py:83\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     81\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\nest_asyncio.py:106\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m     heappop(scheduled)\n\u001b[0;32m    101\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    104\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 106\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    109\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\selectors.py:324\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pysyft\\lib\\selectors.py:315\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 315\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 9\n",
    "batch_size = 8\n",
    "\n",
    "train_loss1_list, valid_loss1_list, train_loss2_list, valid_loss2_list = federated_traing(local_train,epochs ,duet1,duet2, \n",
    "                                                                                          train_input_dangjin, train_out_ptr_dangjin,\n",
    "                                                                                          train_input_ulsan, train_out_ptr_ulsan,\n",
    "                                                                                          valid_input_dangjin, valid_out_ptr_dangjin,\n",
    "                                                                                          valid_input_ulsan, valid_out_ptr_ulsan,\n",
    "                                                                                          test_input_dangjin, test_out_ptr_dangjin,\n",
    "                                                                                          test_input_ulsan, test_out_ptr_ulsan,\n",
    "                                                                                          batch_size,'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss2_list)\n",
    "plt.title(\"ulsan train loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ", valid_loss1_list, train_loss2_list, valid_loss2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4772b214d63be1d4c936648af70aae0e091ab9e88c72d0cce677d17807ffc9f7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
